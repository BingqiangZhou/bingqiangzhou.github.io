---
title: 【论文阅读笔记】基于深度度量学习的语义实例分割（二）
tags: [CV, Semantic Instance Segmentation, Deep Metric Learning]
mathjax: true
mathjax_autoNumber: true
---

接过上一次的话题，这一次，结合自己的理解，记录一下论文中提出的方法，包括嵌入模型（embedding model）、创建遮罩（creating masks）、分类和选种子点模型（classification and seediness model）。

## 嵌入模型（embedding model）
学习一个嵌入模型（维度为[h, w, d]的张量），使用logistics损失来训练它。
### 相似度
$$\sigma(p,q)=\frac{2}{1+exp(||e_p-e_q||)}$$

其中，$\sigma(p,q)$ 表示点p与点q的相似度，当两者在嵌入空间（$e_p$ 与 $e_q$）比较近时，$\sigma(p,q)=frac{2}{1+e^0}=1$，两者比较远时，$\sigma(p,q)=\frac{2}{1+e^\infty}=0$。
### 损失函数
$$L_e=-\frac{1}{|S|}\sum_{{p,q}\in S}w_{pq}[1_{\{y_p=y_q\}}log(\sigma(p,q))+1_{\{y_p\neq y_q\}}log(1-\sigma(p,q))]$$

其中$S$ 是"种子点"集合，这里的$|S|$ 表示集合中种子点的个数，$w_{pq}$ 是点p与点q相似度损失的权重，$w_{pq}$ 与点p和点q所属的实例大小成反比，添加这个权重，从而使得损失函数不会偏向于更大的样本。
$1_{\{y_p=y_q\}}$ 表示当$y_p=y_q$ 成立的时候式子取1，不成立则取0，$1_{\{y_p\neq y_q\}}$ 同理。

论文中还提到了正则化权重，$\sum_{p,q}w_pq=1$ 。

在训练开始时，在每一个实例中随机取样，选取K个点来作为种子点，假设有N个实例，则种子点集合元素个数$|S|=N \cdot K$，在这里只计算在$|S|$中的点与点之间的相似度损失，随后选取种子点的机制，在[分类和选种子点模型](#分类和选种子点模型)说明。


## 创建遮罩
写上m（p，t）公式，解释，当p点与种子点q的相似度大于一个阈值时，则将这个点p设置为与点q同类，反之属于背景， 由此可知每一个种子点都会产生一个遮罩，那么我们如何选种子点呢？首先选择每个类中一个“种子度”最高的点，这个点是在阈值t下属于c类中最大的可能性，加上三个公式，

## 分类和选种子点模型
学习模型，使用softmax交叉熵损失来训练模型，附上公式，解释相关标记

当我们已经选择了最好的种子点集之后，我们就可以算出对应最好的阈值和在选择最好的阈值的情况下相关的置信度。
附上公式。

## 总结与感受
我个人认为论文给我们最重要的方法是自动选取种子点的方法，这样就可以实现自动化分割实体，在应用上，比如重上色，其实选点的过程下可以让用户交互，用户都只要交互一次，在训练上的话，难度也比较小，当然如何我们还是让用户去交互的话，应用面就被限制了，再回到用户交互的方式的话那就是一种退步。
回到方法，感觉还是蛮难的，可能还没有完全搞懂，可能是没有跑代码的原因吧，不过现在大概知道了这个思想了，以后有需要再回顾吧。
